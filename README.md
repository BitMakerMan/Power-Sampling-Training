# Power Sampling - Improve LLM Reasoning

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

A Python implementation of Power Sampling algorithm for improving logical reasoning and consistency in Large Language Models (LLMs) using Metropolis-Hastings autoregressive method.

## üéØ What is Power Sampling?

Power Sampling is an advanced algorithm that improves the quality and coherence of text generated by language models. It works by:

1. **Generating** an initial text sequence
2. **Resampling** text blocks using Metropolis-Hastings
3. **Accepting** only improvements that enhance coherence
4. **Iterating** to achieve optimal quality

**Result**: More coherent, accurate, and logically structured text output.

## üß† How Power Sampling Works - Understanding the Algorithm

### Complete Algorithm Flow
![Power Sampling Algorithm](Doc/PowerSampling_Diagram.png)

### üî¨ The Metropolis-Hastings Method

Power Sampling uses the Metropolis-Hastings algorithm (a technique from computational statistics) to iteratively improve text quality:

#### **Step 1: Initial Generation**
- Generate text normally with the LLM
- This gives us a starting point for improvement

#### **Step 2: Block Resampling**
- Split the text into smaller blocks (typically 16-128 tokens)
- For each block, propose alternative text
- Calculate probability scores for both original and new text

#### **Step 3: Metropolis-Hastings Decision**
- Accept the new text IF it improves overall probability
- Sometimes accept worse text to explore possibilities
- This helps avoid getting stuck in local optima

#### **Step 4: Iteration**
- Repeat the process multiple times
- Each iteration typically improves the text
- More iterations = better quality (but slower)

### Mathematical Process
![Mathematical Flow](Doc/_Mathematical_Flow.png)

### üìä Mathematical Foundation

The acceptance probability is calculated as:

```
P(accept) = min(1, exp(Œ± * (log P_proposal - log P_current)))
```

Where:
- `Œ±` is the sharpening factor (typically 2.0-6.0)
- `P_proposal` and `P_current` are the probabilities

### üéØ Why It Works

- **Focus**: The "alpha" parameter sharpens probabilities, making better text more likely
- **Exploration**: Sometimes accepts worse options to find better solutions
- **Coherence**: Evaluates text in context, not just word by word
- **Iteration**: Multiple rounds of refinement

## üìã Key Parameters

| Parameter | Range | Recommended | Description |
|-----------|--------|-------------|-------------|
| `alpha` | 2.0 - 6.0 | 4.0 | Sharpening factor for focus |
| `block_size` | 16 - 128 | 64 | Text block size for resampling |
| `steps` | 1 - 10 | 5 | Number of Metropolis-Hastings iterations |
| `max_len` | 128 - 512 | 256 | Maximum output length |
| `temperature` | 0.5 - 2.0 | 1.0 | Sampling temperature |

## üöÄ Quick Start Guide

### Installation

```bash
# Clone the original repository
git clone https://github.com/aakaran/reasoning-with-sampling.git
cd reasoning-with-sampling

# Install dependencies
pip install -r requirements.txt

# Install the package
pip install -e .
```

### Basic Usage

```python
from src.power_sampling import load_model_and_tokenizer, power_sample

# Load model (using a lightweight model for demonstration)
model, tokenizer = load_model_and_tokenizer("EleutherAI/gpt-neo-125M")

# Generate improved text
response = power_sample(
    model=model,
    tokenizer=tokenizer,
    prompt="What is artificial intelligence?",
    alpha=4.0,          # Sharpening factor (recommended: 4.0)
    block_size=64,      # Block size for resampling
    steps=5,            # Number of iterations (recommended: 5)
    max_len=200,        # Maximum sequence length
    show_progress=True
)

print(response)
```

### Using the Class Interface

```python
from src.power_sampling import PowerSampler, load_model_and_tokenizer

# Load model and create sampler
model, tokenizer = load_model_and_tokenizer("EleutherAI/gpt-neo-125M")
sampler = PowerSampler(model, tokenizer)

# Generate improved text
response = sampler.power_sample(
    prompt="Explain machine learning simply",
    alpha=4.0,
    steps=5,
    show_progress=True
)

print(response)
```

### Command Line Interface

```bash
# Install and run
pip install -e .
power-sampling
```

## üéØ Performance Tips

### For Quality (Recommended)
```python
response = power_sample(
    model, tokenizer, prompt,
    alpha=4.0,
    steps=5,
    block_size=64
)
```

### For Speed
```python
response = power_sample(
    model, tokenizer, prompt,
    alpha=2.0,
    steps=2,
    block_size=32
)
```

### For Maximum Quality
```python
response = power_sample(
    model, tokenizer, prompt,
    alpha=5.0,
    steps=8,
    block_size=96
)
```

## üìö Educational Examples

The `examples/` directory contains comprehensive educational demos to help you understand Power Sampling:

### 1. Complete Educational Demo (`examples/understand_power_sampling.py`)
**Perfect for learning!** This interactive demo shows everything:
- ‚úÖ Standard vs Power Sampling comparison
- ‚úÖ Different parameter effects (alpha values)
- ‚úÖ Step-by-step process visualization
- ‚úÖ Multiple prompt examples
- ‚úÖ Detailed theory explanations
- ‚úÖ Model selection with 6 pre-configured options
- ‚úÖ Works with local models for offline usage

```bash
cd examples
python understand_power_sampling.py
# Select model (1-7) when prompted
```

### 2. Multi-Model Test (`examples/understand_power_sampling_multimodel_test.py`)
**Automated testing across multiple models** - This script runs the same educational demo with automatic model selection for testing purposes:
- Tests different model performance
- Compares Power Sampling effectiveness across models
- Demonstrates parameter adaptation based on model size
- No user input required - fully automated

```bash
cd examples
python understand_power_sampling_multimodel_test.py
```

### 3. Model Selection Test (`examples/test_model_selection.py`)
**Quick validation script** - Tests the model selection functionality:
- Validates local model detection
- Tests HuggingFace download capability
- Demonstrates automatic parameter adjustment
- Fast testing with minimal output

```bash
cd examples
python test_model_selection.py
```

### Example Features

All examples include:
- **Interactive Learning**: Step-by-step explanations of the Metropolis-Hastings algorithm
- **Real Demonstrations**: See Power Sampling improving text quality in real-time
- **Parameter Guidance**: Understand how alpha, steps, and block_size affect results
- **Multiple Models**: Test with different LLM sizes to see performance differences
- **Error Handling**: Graceful fallbacks when models encounter issues
- **Educational Theory**: Clear explanations of why Power Sampling works

### Model Performance Comparison

Try different models to see how Power Sampling effectiveness varies:

| Model | Parameters | Power Sampling Benefit |
|-------|------------|----------------------|
| GPT-2 (124M) | 124M | Basic improvement |
| GPT-Neo 125M | 125M | Moderate improvement |
| DialoGPT Medium | 345M | Good coherence boost |
| TinyLlama 1.1B | 1.1B | **Best for education** |
| BLOOM-560M | 560M | Strong reasoning gains |
| OPT-1.3B | 1.3B | Excellent refinement |

**Recommendation**: Use TinyLlama 1.1B for the best educational experience - it shows clear Power Sampling improvements while remaining fast enough for interactive demos.

### Model Selection & Performance

The educational demo now supports multiple models from HuggingFace:

**Available Models:**
1. **GPT-2 (124M)** - Fast, basic reasoning
2. **GPT-Neo 125M** - Better logic (default)
3. **DialoGPT Medium (345M)** - Conversational
4. **TinyLlama 1.1B** - Small but capable
5. **BLOOM-560M** - Good reasoning
6. **OPT-1.3B** - Open Pretrained
7. **Custom model** - Any HuggingFace model

**Usage:**
```bash
cd examples
python understand_power_sampling.py
# Select model when prompted (1-7)
```

### Standard vs Power Sampling
![Comparison](Doc/PowerSampling_Comparison.png)

### Example Output Comparison

**Standard LLM Generation (125M model):**
```
What is artificial intelligence?
It depends on the internet. On mobile, for example, it can be seen as a platform to develop new technology...
```

**Power Sampling (Improved):**
```
What is artificial intelligence?
By default, the most advanced form of artificial intelligence (AI) is computer vision, in...
```

**Note:** Small models (125M parameters) have limited reasoning capabilities. For better results, use larger models like TinyLlama 1.1B or OPT-1.3B when testing Power Sampling improvements.

### ‚ö° **Important Reality Check: What Power Sampling Can and Cannot Do**

**üéØ Conclusion from Realistic Testing:**

Results vary **dramatically** depending on the model used. Power Sampling **is not magic** - if the model is limited, you can't get blood from a stone.

**üß† Power Sampling IS:**
- ‚úÖ **Iterative coherence filter** that improves what the model already knows
- ‚úÖ **Anti-stupidity guard** that prevents completely illogical responses
- ‚úÖ **Enhancer** for "knowledgeable" models that already have baseline capabilities
- ‚úÖ **Refiner** that makes text more structured and coherent

**üö´ Power Sampling is NOT:**
- ‚ùå **Artificial intelligence** that turns stupid ‚Üí smart
- ‚ùå **Extra knowledge** that teaches new things to the model
- ‚ùå **Magic pill** that transforms a limited model into an expert
- ‚ùå **Universal solution** for all reasoning problems

**üí° The Truth:**
Power Sampling cannot make a limited model "intelligent," but it can **prevent it from being "stupid"** üôÇ. For models with baseline capabilities (1B+ parameters), it becomes a powerful tool. For very small models (125M), it mainly helps avoid worse responses.

**üé™ Recommended Testing:**
- **125M models**: Expect marginal improvements, mainly coherence
- **345M+ models**: Start seeing real reasoning benefits
- **1B+ models**: **Maximum benefit** - where Power Sampling truly shines

## üèóÔ∏è Project Structure

```
PowerSampling/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Package initialization
‚îÇ   ‚îú‚îÄ‚îÄ power_sampling.py        # Core PowerSampling algorithm
‚îÇ   ‚îî‚îÄ‚îÄ cli.py                   # Command line interface
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ understand_power_sampling.py           # Complete educational demo
‚îÇ   ‚îú‚îÄ‚îÄ understand_power_sampling_multimodel_test.py  # Multi-model testing
‚îÇ   ‚îú‚îÄ‚îÄ test_model_selection.py                # Model selection validation
‚îÇ   ‚îî‚îÄ‚îÄ understand_power_sampling_old.py       # Backup version
‚îú‚îÄ‚îÄ Doc/
‚îÇ   ‚îú‚îÄ‚îÄ PowerSampling_Diagram.png              # Complete algorithm flow
‚îÇ   ‚îú‚îÄ‚îÄ PowerSampling_Mathematical_Flow.png     # Mathematical process
‚îÇ   ‚îî‚îÄ‚îÄ PowerSampling_Comparison.png            # Standard vs Power Sampling
‚îú‚îÄ‚îÄ models--EleutherAI--gpt-neo-125M/          # Local model cache (optional)
‚îú‚îÄ‚îÄ setup.py                                        # Package setup
‚îú‚îÄ‚îÄ requirements.txt                               # Dependencies
‚îî‚îÄ‚îÄ README.md                                      # This file
```

## üìä Use Cases

PowerSampling is particularly effective for:

- üìö **Educational Content**: Create clear, accurate explanations
- üî¨ **Research Papers**: Generate coherent academic writing
- üíº **Business Documents**: Professional reports and communications
- üè• **Medical Content**: Accurate healthcare information
- üì∞ **Journalism**: Balanced, well-structured articles
- ü§ñ **Technical Documentation**: Clear software documentation

## üõ†Ô∏è Requirements

- Python 3.8 or higher
- PyTorch 2.0.0 or higher
- Transformers 4.30.0 or higher
- 4GB+ RAM for small models
- Internet connection for first-time model download

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

### Development Setup

```bash
# Clone the repository
git clone https://github.com/aakaran/reasoning-with-sampling.git
cd reasoning-with-sampling

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install -e .

# Run educational demo
python examples/understand_power_sampling.py
```

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üë• Credits

**Original Author:** [aakaran](https://github.com/aakaran)
- Created the Power Sampling algorithm implementation
- Original repository: https://github.com/aakaran/reasoning-with-sampling

**Demo/Test Preparation:** [Craicek](https://github.com/BitMakerMan)
- Prepared demonstration version for educational purposes
- Added CLI interface and documentation improvements
- Created comprehensive educational examples
- Set up local model usage for offline testing

## üìû Contact

For questions about this demo version:
- **GitHub:** https://github.com/BitMakerMan/Power-Sampling-Training

For the original project:
- **GitHub:** https://github.com/aakaran
- **Original Repository:** https://github.com/aakaran/reasoning-with-sampling

## üîó References

- [Original Paper](https://arxiv.org/abs/[paper-id])
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [Metropolis-Hastings Algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)

---

## üéâ Summary

**You now have everything to understand and use Power Sampling:**

1. ‚úÖ **Theory**: Complete explanation of Metropolis-Hastings algorithm
2. ‚úÖ **Practice**: Multiple working examples with local model support
3. ‚úÖ **Parameters**: Understanding of how to tune for quality vs speed
4. ‚úÖ **Applications**: Real-world use cases and benefits

**‚≠ê If you find this useful, please give it a star on GitHub!**
